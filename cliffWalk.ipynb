{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4f8696",
   "metadata": {},
   "source": [
    "# Cliff Walking and Q-Learning\n",
    "Cliff Walking Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaeb389",
   "metadata": {},
   "source": [
    "The Cliff Walking environment is a grid world problem commonly used in reinforcement learning. In this environment, an agent must navigate from a start state to a goal state, typically located at opposite corners of a grid. The challenge lies in the presence of 'cliffs' along the way. If the agent falls off a cliff, it incurs a large negative reward and is sent back to the start state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4854df99",
   "metadata": {},
   "source": [
    "# Q-Learning:\n",
    "\n",
    "Q-learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for a given finite Markov decision process. It aims to learn a policy, which tells an agent what action to take under what circumstances. The agent learns to estimate the value of actions taken in states (Q-values) based on the rewards it receives for its actions. Over time, the agent learns the best actions to take in each state to maximize its cumulative reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39de081",
   "metadata": {},
   "source": [
    "# Imports:\n",
    "\n",
    "`import gymnasium as gym:` Imports the Gymnasium library, which provides the Cliff Walking environment. Gymnasium is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "`import numpy as np:` Imports the NumPy library, used for numerical operations like handling arrays (such as the Q-table).\n",
    "`import random:` Imports the random library to perform `random` selections, which is crucial for the exploration part of the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f332b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549461ee",
   "metadata": {},
   "source": [
    "# Function Definition: \n",
    "\n",
    "This line defines a function named `q_learning_epsilon_constant` with parameters `it_max`, `epsilon`, `learning_rate`, `discount_factor`, and an optional parameter `render` (defaulting to `False`). Each of these parameters plays a role in configuring the Q-learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee0dc2",
   "metadata": {},
   "source": [
    "# Environment Initialization:\n",
    "\n",
    "Here, the `Cliff Walking environment` is created using Gymnasium. The `render_mode` is set based on the render parameter. If `render` is `True`, the environment will display its graphical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9a7f38",
   "metadata": {},
   "source": [
    "# Defining States and Actions:\n",
    "\n",
    "These lines fetch the number of states (`num_states`) and actions (`num_actions`) in the Cliff Walking environment. This information is used to structure the Q-table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c3520",
   "metadata": {},
   "source": [
    "# Initializing Q-Table and Rewards Table:\n",
    "\n",
    "The Q-table (`q_table`) is initialized with zeros and has a size equal to the number of states times the number of actions. The `rewards_table` is an array to keep track of the total rewards received in each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109168e0",
   "metadata": {},
   "source": [
    "# Episode Iteration:\n",
    "\n",
    "This loop iterates over the number of episodes (`it_max`). For each episode, the environment is reset, and variables like `terminated`, `truncated`, and `total_rewards` are initialized. `observation` holds the initial state of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0400e03d",
   "metadata": {},
   "source": [
    "# Action Selection and Environment Step:\n",
    "\n",
    "Inside each episode, the algorithm runs until the state is either `terminated` or `truncated` (like reaching a terminal state or falling off a cliff). It decides whether to take a random action (exploration, based on `epsilon`) or the best-known action (exploitation, based on the max Q-value for the current state). The environment then progresses to the next state (`next_observation`) with the selected action, returning the reward and new state information. This reward is added to the `total_rewards` for the episode.\n",
    "\n",
    "These code sections collectively set up and run the Q-learning algorithm in the Cliff Walking environment, handling the learning process and interactions with the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23bb9c5",
   "metadata": {},
   "source": [
    "# Q-Learning Update:\n",
    "\n",
    "`diff = reward + discount_factor * np.max(q_table[next_observation]) - q_table[observation, action]\n",
    "q_table[observation][action] += learning_rate * diff\n",
    "`\n",
    "This block of code is the core of the Q-learning algorithm. For each step in an episode, it updates the Q-value for the current state-action pair. The update rule is based on the Bellman equation, where:\n",
    "\n",
    "`reward:` The immediate reward received after performing the action.\n",
    "`discount_factor * np.max(q_table[next_observation]):` The maximum predicted Q-value for the next state, weighted by the discount factor.\n",
    "`q_table[observation, action]:` The current Q-value for the state-action pair.\n",
    "`diff:` The difference between the estimated Q-value and the observed Q-value (the temporal difference error).\n",
    "`learning_rate:` The rate at which the Q-table is updated. A higher learning rate means the algorithm quickly adopts new Q-values.\n",
    "\n",
    "# Update State and Episode Handling:\n",
    "\n",
    "`observation = next_observation\n",
    "rewards_table[episode] = total_rewards\n",
    "`\n",
    "After the Q-table update, the current observation is updated to the next observation. At the end of each episode, the total rewards for that episode are stored in the `rewards_table`.\n",
    "\n",
    "# Optimal Episode Check:\n",
    "\n",
    "This part checks if the agent has achieved the best possible outcome for the first time. In the Cliff Walking environment, `-13` is typically the highest possible total reward (least negative). If this is achieved for the first time, the episode number is stored as the `optimal_episode`.\n",
    "\n",
    "# Rendering and Closing the Environment:\n",
    "\n",
    "`if render:\n",
    "    env.render()\n",
    "env.close()\n",
    "`\n",
    "If rendering is enabled, the environment is rendered after each episode. Finally, the environment is closed, which is important for resource cleanup.\n",
    "\n",
    "# Return Statement:\n",
    "\n",
    "`return q_table, rewards_table, optimal_episode\n",
    "`\n",
    "The function returns the learned Q-table, the rewards for each episode, and the first episode when the optimal path is found.\n",
    "\n",
    "# Function Execution and Output:\n",
    "\n",
    "Parameters for the Q-learning algorithm are defined (`it_max`, `epsilon`, `learning_rate`, `discount_factor`).\n",
    "The Q-learning function is executed with these parameters.\n",
    "The learned Q-table, rewards for each episode, the first optimal episode, the average reward, and the percentage of successful episodes are printed.\n",
    "This code thus comprehensively covers the implementation and execution of the Q-learning algorithm in the Cliff Walking environment, including the learning process, environment interaction, and performance evaluation.\n",
    "\n",
    "# Defining Parameters:\n",
    "\n",
    "`it_max = 300\n",
    "epsilon = 0.1\n",
    "learning_rate = 0.9\n",
    "discount_factor = 0.9\n",
    "`\n",
    "Here, you are setting up the parameters for the Q-learning algorithm.\n",
    "\n",
    "`it_max:` The maximum number of episodes for training the Q-learning agent.\n",
    "`epsilon:` The exploration rate. This is a probability value that dictates how often the agent will choose a random action over what it believes is the best action. This aids in exploring the state space.\n",
    "`learning_rate:` Determines how much the Q-value is updated during learning. A higher value means faster learning, but can be less stable.\n",
    "`discount_factor:` Used to balance immediate and future rewards. A value closer to 1 places more importance on future rewards.\n",
    "\n",
    "# Executing Q-Learning:\n",
    "\n",
    "`q_table, rewards_table, optimal_episode = q_learning_epsilon_constant(it_max, epsilon, learning_rate, discount_factor, render=True)\n",
    "`\n",
    "This line calls the `q_learning_epsilon_constant` function with the parameters you've defined. It also sets `render=True`, which means the algorithm's progress will be visually rendered. This function returns:\n",
    "\n",
    "`q_table:` The final Q-table after training, representing the learned values for each action in each state.\n",
    "`rewards_table:` An array containing the total reward accumulated in each episode.\n",
    "`optimal_episode:` The first episode in which the agent achieved the best possible total reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775d440",
   "metadata": {},
   "source": [
    "# Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def q_learning_epsilon_constant(it_max, epsilon, learning_rate, discount_factor, render=False):\n",
    "    env = gym.make('CliffWalking-v0', render_mode=\"human\" if render else None)\n",
    "    num_states = env.observation_space.n\n",
    "    num_actions = env.action_space.n\n",
    "    q_table = np.zeros((num_states, num_actions))\n",
    "    rewards_table = np.zeros(it_max)\n",
    "    optimal_episode = None\n",
    "\n",
    "    for episode in range(it_max):\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "        total_rewards = 0\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        while not terminated and not truncated:\n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if random.random() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(q_table[observation])\n",
    "            \n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "\n",
    "            # Q-learning update\n",
    "            diff = reward + discount_factor * np.max(q_table[next_observation]) - q_table[observation, action]\n",
    "            q_table[observation][action] += learning_rate * diff\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "        rewards_table[episode] = total_rewards\n",
    "\n",
    "        if total_rewards == -13 and optimal_episode is None:\n",
    "            optimal_episode = episode\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    env.close()\n",
    "    return q_table, rewards_table, optimal_episode\n",
    "\n",
    "# Define your parameters\n",
    "it_max = 300\n",
    "epsilon = 0.1\n",
    "learning_rate = 0.9\n",
    "discount_factor = 0.9\n",
    "\n",
    "# Perform Q-learning with rendering\n",
    "q_table, rewards_table, optimal_episode = q_learning_epsilon_constant(it_max, epsilon, learning_rate, discount_factor, render=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87bae22",
   "metadata": {},
   "source": [
    "# Notice:\n",
    "To show the `percent` of the rewards and success we can write `\"rgb_array\"` instead of the `\"human\"` in `render_mode`!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
